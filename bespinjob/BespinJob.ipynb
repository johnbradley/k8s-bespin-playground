{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from cluster import ClusterApi, BatchJobSpec, SecretVolume, PersistentClaimVolume, ConfigMapVolume, Container\n",
    "# turn off warnings due to dev cluster cert\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'volume-project'\n",
    "job_name = \"job8\"\n",
    "job_claim_name = \"{}-claim\".format(job_name)\n",
    "job_secret_name = \"{}-dukeds\".format(job_name)\n",
    "\n",
    "stage_job_config_map_name = \"{}-stage-config\".format(job_name)\n",
    "stage_job_name = \"{}-stage\".format(job_name)\n",
    "\n",
    "download_workflow_job_name = \"{}-down-wf\".format(job_name)\n",
    "\n",
    "cwl_job_config_map_name = \"{}-cwl-config\".format(job_name)\n",
    "cwl_job_name = \"{}-cwl\".format(job_name)\n",
    "\n",
    "save_results_job_config_map_name = \"{}-save-config\".format(job_name)\n",
    "save_results_job_name = \"{}-save\".format(job_name)\n",
    "save_results_project_name = \"results_from_job8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ClusterApi(namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PVC to hold data for the job\n",
    "claim = api.create_persistent_volume_claim(job_claim_name, storage_size_in_g=2)\n",
    "persistent_claim_volume = PersistentClaimVolume(\"data\", mount_path=\"/data\", volume_claim_name=job_claim_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Secret to hold DukeDSClient keys\n",
    "agent_key = 'TODO'\n",
    "user_key = 'TODO'\n",
    "ddsclient_secret_config = json.dumps({\"agent_key\": agent_key, \"user_key\": user_key})\n",
    "secret = api.create_secret(job_secret_name, {\"config\": ddsclient_secret_config})\n",
    "ddsclient_secret_volume = SecretVolume(\"ddsclient\", mount_path=\"/etc/ddsclient\", secret_name=job_secret_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record config map of files to be downloaded\n",
    "# TODO: make format more generic\n",
    "config_data = {\n",
    "  \"files\": [\n",
    "    {\n",
    "      \"key\": \"TODO\",\n",
    "      \"dest\": \"data.txt\"\n",
    "    },\n",
    "    {\n",
    "      \"key\": \"TODO\",\n",
    "      \"dest\": \"SRR_1_first_100000.fastq.gz\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "payload = {\n",
    "    \"commands\": json.dumps(config_data)\n",
    "}\n",
    "config_map = api.create_config_map(name=stage_job_config_map_name, data=payload)\n",
    "stage_data_config_volume = ConfigMapVolume(\"config\", mount_path=\"/etc/config\", \n",
    "                                           config_map_name=stage_job_config_map_name, \n",
    "                                           source_key=\"commands\", source_path=\"commands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run job to stage data based on the config map\n",
    "container = Container(\n",
    "    name=stage_job_name, \n",
    "    image_name=\"jbradley/duke-ds-staging\", \n",
    "    command=\"python\", \n",
    "    args=[\"/app/download.py\", \"/etc/config/commands\"], \n",
    "    working_dir=\"/data\", \n",
    "    env_dict={\"DDSCLIENT_CONF\": \"/etc/ddsclient/config\"},\n",
    "    requested_cpu=\"100m\",\n",
    "    requested_memory=\"64Mi\",\n",
    "    volumes=[\n",
    "        persistent_claim_volume,    \n",
    "        ddsclient_secret_volume, \n",
    "        stage_data_config_volume,\n",
    "    ])\n",
    "job_spec = BatchJobSpec(stage_job_name, container=container)\n",
    "job = api.create_job(stage_job_name, job_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs complete: job8-stage\n"
     ]
    }
   ],
   "source": [
    "# Wait then cleanup job\n",
    "api.wait_for_jobs(job_names=[stage_job_name])\n",
    "api.delete_job(stage_job_name)\n",
    "api.delete_config_map(stage_job_config_map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs complete: job8-down-wf\n"
     ]
    }
   ],
   "source": [
    "# Download workflow\n",
    "container = Container(\n",
    "    name=download_workflow_job_name, \n",
    "    image_name=\"jbradley/duke-ds-staging\", \n",
    "    command=\"wget\", \n",
    "    args=[\"https://raw.githubusercontent.com/Duke-GCB/bespin-cwl/master/tools/fastqc.cwl\"],\n",
    "    working_dir=\"/data\", \n",
    "    env_dict={},\n",
    "    requested_cpu=\"100m\",\n",
    "    requested_memory=\"64Mi\",\n",
    "    volumes=[persistent_claim_volume]\n",
    ")\n",
    "job_spec = BatchJobSpec(download_workflow_job_name, container=container)\n",
    "job = api.create_job(download_workflow_job_name, job_spec)\n",
    "# Wait then cleanup\n",
    "api.wait_for_jobs(job_names=[download_workflow_job_name])\n",
    "api.delete_job(download_workflow_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "  \"input_fastq_file\": {\n",
    "      \"class\": \"File\",\n",
    "      \"path\": \"/data/SRR_1_first_100000.fastq.gz\",\n",
    "  }\n",
    "}\n",
    "payload = {\n",
    "    \"joborder\": json.dumps(config_data)\n",
    "}\n",
    "config_map = api.create_config_map(name=cwl_job_config_map_name, data=payload)\n",
    "joborder_volume = ConfigMapVolume(\"config\", \n",
    "                    mount_path=\"/etc/config\", \n",
    "                    config_map_name=cwl_job_config_map_name, \n",
    "                    source_key=\"joborder\", \n",
    "                    source_path=\"joborder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run job to create taco.txt\n",
    "container = Container(\n",
    "    name=cwl_job_name, \n",
    "    image_name=\"jbradley/cwltool-fastqc\", \n",
    "    command=\"cwltool\", \n",
    "    args=[\"--no-container\", \"--outdir\", \"/data/results\", \"fastqc.cwl\", \"/etc/config/joborder\"],\n",
    "    working_dir=\"/data\", \n",
    "    env_dict={},\n",
    "    requested_cpu=\"100m\",\n",
    "    requested_memory=\"64Mi\",\n",
    "    volumes=[persistent_claim_volume, joborder_volume])\n",
    "job_spec = BatchJobSpec(cwl_job_name, container=container)\n",
    "job = api.create_job(cwl_job_name, job_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs complete: job8-cwl\n"
     ]
    }
   ],
   "source": [
    "# Wait then cleanup\n",
    "api.wait_for_jobs(job_names=[cwl_job_name])\n",
    "api.delete_config_map(cwl_job_config_map_name)\n",
    "api.delete_job(cwl_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record config map of files to be uploaded\n",
    "# TODO: make format more generic\n",
    "config_data = {\n",
    "    \"destination\": save_results_project_name,\n",
    "    \"paths\": [ \"/data/results\" ]\n",
    "}\n",
    "payload = {\n",
    "    \"commands\": json.dumps(config_data)\n",
    "}\n",
    "config_map = api.create_config_map(name=save_results_job_config_map_name, data=payload)\n",
    "save_results_config_volume = ConfigMapVolume(\"config\", mount_path=\"/etc/config\", \n",
    "                                             config_map_name=save_results_job_config_map_name, \n",
    "                                             source_key=\"commands\", source_path=\"commands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run job to store output to a project\n",
    "container = Container(\n",
    "    name=save_results_job_name, \n",
    "    image_name=\"jbradley/duke-ds-staging\", \n",
    "    command=\"python\", \n",
    "    args=[\"/app/upload.py\", \"/etc/config/commands\"], \n",
    "    working_dir=\"/data\", \n",
    "    env_dict={\"DDSCLIENT_CONF\": \"/etc/ddsclient/config\"},\n",
    "    requested_cpu=\"100m\",\n",
    "    requested_memory=\"64Mi\",\n",
    "    volumes=[ddsclient_secret_volume, persistent_claim_volume, save_results_config_volume])\n",
    "job_spec = BatchJobSpec(save_results_job_name, container=container)\n",
    "job = api.create_job(save_results_job_name, job_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs complete: job8-save\n"
     ]
    }
   ],
   "source": [
    "# After stage job finishes cleanup\n",
    "api.wait_for_jobs(job_names=[save_results_job_name])\n",
    "api.delete_job(save_results_job_name)\n",
    "api.delete_config_map(save_results_job_config_map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup job volume and secret\n",
    "api.delete_persistent_volume_claim(job_claim_name)\n",
    "api.delete_secret(job_secret_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
